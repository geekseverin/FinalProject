"""
Module d'extraction des r√®gles d'association
"""

import pandas as pd
import numpy as np
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
import joblib
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent))
from config.config import *

class AssociationRulesAnalyzer:
    def __init__(self):
        self.frequent_itemsets = None
        self.rules = None
        self.recommendations = {}
        
    def load_processed_data(self):
        """Charge les donn√©es pr√©process√©es"""
        try:
            data_path = PROCESSED_DATA_DIR / "cleaned_data.csv"
            return pd.read_csv(data_path)
        except Exception as e:
            print(f"Erreur lors du chargement des donn√©es: {e}")
            return None
    
    def prepare_transaction_data(self, df):
        """Pr√©paration des donn√©es pour l'analyse des r√®gles d'association"""
        print("üîÑ Pr√©paration des donn√©es transactionnelles...")
        
        # Cr√©ation des bins pour les variables continues
        df_binned = df.copy()
        
        # Binning des variables continues
        df_binned['age_group'] = pd.cut(df['age'], bins=[0, 25, 30, 100], labels=['Young', 'Middle', 'Senior'])
        df_binned['grade_level'] = pd.cut(df['average_grade'], bins=[0, 10, 15, 20], labels=['Low_Grade', 'Medium_Grade', 'High_Grade'])
        df_binned['absenteeism_level'] = pd.cut(df['absenteeism_rate'], bins=[0, 10, 20, 100], labels=['Low_Absent', 'Medium_Absent', 'High_Absent'])
        df_binned['satisfaction_level'] = pd.cut(df['satisfaction_score'], bins=[0, 3, 6, 10], labels=['Low_Satisfaction', 'Medium_Satisfaction', 'High_Satisfaction'])
        df_binned['forum_activity'] = pd.cut(df['forum_posts'], bins=[0, 2, 5, 100], labels=['Low_Forum', 'Medium_Forum', 'High_Forum'])
        df_binned['moodle_usage'] = pd.cut(df['moodle_hours'], bins=[0, 5, 15, 100], labels=['Low_Moodle', 'Medium_Moodle', 'High_Moodle'])
        
        # Conversion en format transactionnel
        categorical_columns = [
            'gender', 'region', 'parent_education', 'age_group', 
            'grade_level', 'absenteeism_level', 'satisfaction_level',
            'forum_activity', 'moodle_usage', 'dropout'
        ]
        
        # Filtrage des colonnes existantes
        existing_columns = [col for col in categorical_columns if col in df_binned.columns]
        df_categorical = df_binned[existing_columns]
        
        # Conversion des valeurs en cha√Ænes et ajout de pr√©fixes
        transactions = []
        for _, row in df_categorical.iterrows():
            transaction = []
            for col in df_categorical.columns:
                if pd.notna(row[col]):
                    if col == 'dropout':
                        value = 'Dropout_Yes' if row[col] == 1 or row[col] == 'Yes' else 'Dropout_No'
                    else:
                        value = f"{col}_{row[col]}"
                    transaction.append(value)
            transactions.append(transaction)
        
        # Encodage des transactions
        te = TransactionEncoder()
        te_ary = te.fit_transform(transactions)
        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)
        
        return df_encoded, transactions
    
    def extract_frequent_itemsets(self, df_encoded):
        """Extraction des itemsets fr√©quents"""
        print("üîÑ Extraction des itemsets fr√©quents...")
        
        # Application de l'algorithme Apriori
        self.frequent_itemsets = apriori(
            df_encoded, 
            min_support=MODEL_CONFIG['min_support'], 
            use_colnames=True
        )
        
        if len(self.frequent_itemsets) == 0:
            print("‚ö†Ô∏è  Aucun itemset fr√©quent trouv√©. R√©duction du seuil de support.")
            self.frequent_itemsets = apriori(
                df_encoded, 
                min_support=0.05, 
                use_colnames=True
            )
        
        print(f"‚úÖ {len(self.frequent_itemsets)} itemsets fr√©quents trouv√©s")
        return self.frequent_itemsets
    
    def generate_association_rules(self):
        """G√©n√©ration des r√®gles d'association"""
        print("üîÑ G√©n√©ration des r√®gles d'association...")
        
        if self.frequent_itemsets is None or len(self.frequent_itemsets) == 0:
            print("‚ùå Pas d'itemsets fr√©quents disponibles")
            return None
        
        try:
            # G√©n√©ration des r√®gles
            self.rules = association_rules(
                self.frequent_itemsets,
                metric="confidence",
                min_threshold=MODEL_CONFIG['min_confidence']
            )
            
            if len(self.rules) == 0:
                print("‚ö†Ô∏è  Aucune r√®gle trouv√©e. R√©duction du seuil de confiance.")
                self.rules = association_rules(
                    self.frequent_itemsets,
                    metric="confidence",
                    min_threshold=0.3
                )
            
            # Tri par lift et confiance
            self.rules = self.rules.sort_values(['lift', 'confidence'], ascending=False)
            
            print(f"‚úÖ {len(self.rules)} r√®gles d'association g√©n√©r√©es")
            return self.rules
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration des r√®gles: {e}")
            return None
    
    def extract_dropout_rules(self):
        """Extraction des r√®gles li√©es √† l'abandon scolaire"""
        if self.rules is None or len(self.rules) == 0:
            return None
        
        # Filtrage des r√®gles li√©es au dropout
        dropout_rules = self.rules[
            self.rules['consequents'].astype(str).str.contains('Dropout_Yes') |
            self.rules['antecedents'].astype(str).str.contains('Dropout_Yes')
        ]
        
        # Focus sur les r√®gles pr√©disant l'abandon
        prediction_rules = dropout_rules[
            dropout_rules['consequents'].astype(str).str.contains('Dropout_Yes')
        ]
        
        return prediction_rules
    
    def generate_recommendations(self, dropout_rules):
        """G√©n√©ration des recommandations bas√©es sur les r√®gles"""
        print("üîÑ G√©n√©ration des recommandations...")
        
        recommendations = {
            'high_risk_patterns': [],
            'intervention_suggestions': [],
            'prevention_strategies': []
        }
        
        if dropout_rules is None or len(dropout_rules) == 0:
            # Recommandations g√©n√©riques si pas de r√®gles sp√©cifiques
            recommendations['intervention_suggestions'] = [
                "Mettre en place un suivi personnalis√© pour les √©tudiants √† risque",
                "Encourager la participation aux forums de discussion",
                "Organiser des sessions de rattrapage pour les √©tudiants en difficult√©",
                "Am√©liorer l'engagement sur la plateforme Moodle"
            ]
            return recommendations
        
        # Analyse des r√®gles pour g√©n√©rer des recommandations
        for _, rule in dropout_rules.head(10).iterrows():
            antecedents = list(rule['antecedents'])
            confidence = rule['confidence']
            lift = rule['lift']
            
            # Extraction des patterns √† risque
            risk_pattern = {
                'conditions': antecedents,
                'dropout_risk': confidence,
                'strength': lift
            }
            recommendations['high_risk_patterns'].append(risk_pattern)
            
            # G√©n√©ration de suggestions d'intervention
            suggestions = self._generate_interventions_from_pattern(antecedents)
            recommendations['intervention_suggestions'].extend(suggestions)
        
        # Suppression des doublons
        recommendations['intervention_suggestions'] = list(set(recommendations['intervention_suggestions']))
        
        self.recommendations = recommendations
        return recommendations
    
    def _generate_interventions_from_pattern(self, pattern):
        """G√©n√©ration d'interventions bas√©es sur un pattern"""
        suggestions = []
        
        for condition in pattern:
            if 'Low_Grade' in condition:
                suggestions.append("Proposer du tutorat acad√©mique et des sessions de r√©vision")
            elif 'High_Absent' in condition:
                suggestions.append("Mettre en place un syst√®me d'alerte pr√©coce pour l'absent√©isme")
            elif 'Low_Satisfaction' in condition:
                suggestions.append("Am√©liorer l'exp√©rience √©tudiante et recueillir des feedbacks")
            elif 'Low_Forum' in condition:
                suggestions.append("Encourager la participation aux discussions en ligne")
            elif 'Low_Moodle' in condition:
                suggestions.append("Organiser des formations sur l'utilisation de Moodle")
            elif 'None' in condition and 'parent_education' in condition:
                suggestions.append("Offrir un accompagnement renforc√© aux √©tudiants de premi√®re g√©n√©ration")
        
        return suggestions
    
    def save_association_results(self):
        """Sauvegarde des r√©sultats d'association"""
        # Sauvegarde des itemsets fr√©quents
        if self.frequent_itemsets is not None:
            joblib.dump(self.frequent_itemsets, ASSOCIATION_RULES_DIR / "frequent_itemsets.pkl")
        
        # Sauvegarde des r√®gles
        if self.rules is not None:
            joblib.dump(self.rules, ASSOCIATION_RULES_DIR / "rules.pkl")
            self.rules.to_csv(OUTPUTS_DIR / "association_rules.csv", index=False)
        
        # Sauvegarde des recommandations
        if self.recommendations:
            joblib.dump(self.recommendations, ASSOCIATION_RULES_DIR / "recommendations.pkl")
    
    def run_association_analysis(self):
        """Pipeline complet d'analyse des r√®gles d'association"""
        print("üîÑ D√©but de l'analyse des r√®gles d'association...")
        
        # Chargement des donn√©es
        df = self.load_processed_data()
        if df is None:
            return None
        
        # Pr√©paration des donn√©es transactionnelles
        df_encoded, transactions = self.prepare_transaction_data(df)
        print(f"‚úÖ Donn√©es transactionnelles pr√©par√©es: {df_encoded.shape}")
        
        # Extraction des itemsets fr√©quents
        frequent_itemsets = self.extract_frequent_itemsets(df_encoded)
        
        # G√©n√©ration des r√®gles d'association
        rules = self.generate_association_rules()
        
        # Extraction des r√®gles li√©es au dropout
        dropout_rules = self.extract_dropout_rules()
        
        # G√©n√©ration des recommandations
        recommendations = self.generate_recommendations(dropout_rules)
        
        # Sauvegarde des r√©sultats
        self.save_association_results()
        
        print("‚úÖ Analyse des r√®gles d'association termin√©e!")
        
        return {
            'frequent_itemsets': frequent_itemsets,
            'rules': rules,
            'dropout_rules': dropout_rules,
            'recommendations': recommendations,
            'n_transactions': len(transactions)
        }

def run_association_rules():
    """Fonction principale pour l'analyse des r√®gles d'association"""
    analyzer = AssociationRulesAnalyzer()
    return analyzer.run_association_analysis()

if __name__ == "__main__":
    run_association_rules()